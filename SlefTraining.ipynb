{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda3f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%run datainfo.ipynb\n",
    "\n",
    "def train_and_evaluate_model(base_model):\n",
    "    # 1. Create a hold-out validation set from train_df only\n",
    "    X_supervised_train, X_val, y_supervised_train, y_val = train_test_split(\n",
    "        train_df.drop('Class/ASD', axis=1),\n",
    "        train_df['Class/ASD'],\n",
    "        test_size=0.4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 2. Prepare self-training data\n",
    "    # Add fake -1 to test_df again to make sure it's still 'unlabeled'\n",
    "    test_df['Class/ASD'] = -1\n",
    "\n",
    "    # Combine the reduced train and test for self-training\n",
    "    train_for_self_training = pd.concat([\n",
    "        X_supervised_train.assign(**{'Class/ASD': y_supervised_train}),\n",
    "        test_df\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    X_combined = train_for_self_training.drop('Class/ASD', axis=1)\n",
    "    y_combined = train_for_self_training['Class/ASD']\n",
    "\n",
    "    # 3. Define and train the self-training model\n",
    "    #base_model = RandomForestClassifier(n_estimators=600, max_depth=10, min_samples_split=10)\n",
    "    self_training_model = SelfTrainingClassifier(base_model, criterion='threshold', k_best=251, threshold=.84)\n",
    "    self_training_model.fit(X_combined, y_combined)\n",
    "\n",
    "    # 4. Evaluate on the real validation set (never seen during self-training)\n",
    "    y_pred_val_self_training = self_training_model.predict(X_val)\n",
    "\n",
    "    f1_self_training = f1_score(y_val, y_pred_val_self_training, average='macro')\n",
    "    f1_supervised = final_f1\n",
    "\n",
    "    # Calculate the uplift\n",
    "    uplift = ((f1_self_training - f1_supervised) / f1_supervised) * 100\n",
    "\n",
    "    print(\"Self-training model Macro-F1 on validation set:\", f1_self_training)\n",
    "    print (\"Supervised model Macro-F1 on validation set:\", f1_supervised)\n",
    "    print(f\"Macro-F1 Uplift/iporivment: {uplift:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c0b10301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-training model Macro-F1 on validation set: 0.8256410256410256\n",
      "Supervised model Macro-F1 on validation set: 0.7164179104477612\n",
      "Macro-F1 Uplift/iporivment: 15.25%\n"
     ]
    }
   ],
   "source": [
    "# random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=600, max_depth=10, min_samples_split=10)\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "31d6e067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-training model Macro-F1 on validation set: 0.8032364012657658\n",
      "Supervised model Macro-F1 on validation set: 0.7164179104477612\n",
      "Macro-F1 Uplift/iporivment: 12.12%\n"
     ]
    }
   ],
   "source": [
    "# random forest model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=1,  # Handle imbalance through class weight\n",
    "    random_state=42\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "922cc2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-training model Macro-F1 on validation set: 0.8009678544880838\n",
      "Supervised model Macro-F1 on validation set: 0.7164179104477612\n",
      "Macro-F1 Uplift/iporivment: 11.80%\n"
     ]
    }
   ],
   "source": [
    "# random forest model\n",
    "cb_model = CatBoostClassifier(\n",
    "    random_state=42, verbose=0\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(cb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5f8c2bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-training model Macro-F1 on validation set: 0.7916666666666667\n",
      "Supervised model Macro-F1 on validation set: 0.7164179104477612\n",
      "Macro-F1 Uplift/iporivment: 10.50%\n"
     ]
    }
   ],
   "source": [
    "# random forest model\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced', random_state=42\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(lr_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
