{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbaaf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f43db0",
   "metadata": {},
   "source": [
    "## ðŸ“Š Self-Training Classifier Evaluation\n",
    "\n",
    "This section demonstrates how to implement and evaluate a **semi-supervised learning pipeline using Self-Training Classifier**.\n",
    "\n",
    "### Objective:\n",
    "Leverage both **labeled (`train_df`) and unlabeled (`test_df`) data** to improve model performance, and quantify the improvement using **Macro-F1 Score uplift**.\n",
    "\n",
    "---\n",
    "\n",
    "### Process Overview:\n",
    "1. **Supervised Split**  \n",
    "   - Split the labeled data (`train_df`) into:\n",
    "     - **Supervised training set** (`X_supervised_train`, `y_supervised_train`).\n",
    "     - **Validation set** (`X_val`, `y_val`).\n",
    "   - Validation set remains **untouched throughout training** and is used only for final evaluation.\n",
    "\n",
    "2. **Self-Training Setup**  \n",
    "   - Assign a placeholder label `-1` to the unlabeled data (`test_df`).\n",
    "   - **Combine supervised data and unlabeled data** into a new dataset for self-training.\n",
    "\n",
    "3. **Model Training**  \n",
    "   - Define a base classifier (e.g., RandomForest, LogisticRegression, CatBoost).\n",
    "   - Automatically wrap models needing scaling in a pipeline with `StandardScaler`.\n",
    "   - Apply `SelfTrainingClassifier` using the base model.\n",
    "   - Train the model, which will:\n",
    "     - Iteratively label the most confident unlabeled samples.\n",
    "     - Re-train on the expanded dataset.\n",
    "\n",
    "4. **Evaluation & Uplift Calculation**  \n",
    "   - Predict on the **unseen validation set** (`X_val`).\n",
    "   - Calculate **Macro-F1 Score**:\n",
    "     - **Macro-F1** gives equal weight to each class, which is crucial for imbalanced datasets.\n",
    "   - Calculate the **uplift (%) over the supervised baseline**:\n",
    "     \\[\n",
    "     \\text{Uplift} = \\frac{F1_{self\\_training} - F1_{supervised}}{F1_{supervised}} \\times 100\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Why Macro-F1 Score?\n",
    "- **Macro-F1** averages F1 scores per class, treating all classes equally.\n",
    "- Essential when **class imbalance exists**, ensuring performance isn't dominated by majority classes.\n",
    "\n",
    "### Key Metric:\n",
    "| Metric    | Description                                    |\n",
    "|-----------|------------------------------------------------|\n",
    "| Macro-F1  | Balanced F1 across all classes (unbiased by class size). |\n",
    "| Uplift %  | Percentage improvement of self-training vs supervised-only model. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path  # Import Path from pathlib\n",
    "# Find project root (the folder containing .git or a marker file)\n",
    "project_root = Path(__file__).resolve().parents[1] if '__file__' in globals() else Path().resolve()\n",
    "os.chdir(project_root)\n",
    "sys.path.append(str(project_root))  # Add to import path\n",
    "\n",
    "# List all files in the current directory\n",
    "files_in_dir = os.listdir(project_root)\n",
    "print(\"Files in current directory:\", files_in_dir)\n",
    "\n",
    "%run datainfo.ipynb\n",
    "\n",
    "score_cols = [f\"A{i}_Score\" for i in range(1, 11)]\n",
    "train_df['total_score'] = train_df[score_cols].sum(axis=1)\n",
    "test_df['total_score'] = test_df[score_cols].sum(axis=1)\n",
    "\n",
    "# Normalize the total score\n",
    "train_df['score_ratio'] = train_df['total_score'] / 10\n",
    "test_df['score_ratio'] = test_df['total_score'] / 10\n",
    "\n",
    "# Add interaction features\n",
    "train_df['gender_result'] = train_df['gender'] * train_df['result']\n",
    "train_df['age_score_ratio'] = train_df['age'] * train_df['score_ratio']\n",
    "train_df['score_autism'] = train_df['total_score'] * train_df['autism']\n",
    "train_df['age_jaundice'] = train_df['age'] * train_df['jaundice']\n",
    "train_df['autism_result'] = train_df['autism'] * train_df['result']\n",
    "train_df['gender_total_score'] = train_df['gender'] * train_df['total_score']\n",
    "\n",
    "test_df['gender_result'] = test_df['gender'] * test_df['result']\n",
    "test_df['age_score_ratio'] = test_df['age'] * test_df['score_ratio']\n",
    "test_df['score_autism'] = test_df['total_score'] * test_df['autism']\n",
    "test_df['age_jaundice'] = test_df['age'] * test_df['jaundice']\n",
    "test_df['autism_result'] = test_df['autism'] * test_df['result']\n",
    "test_df['gender_total_score'] = test_df['gender'] * test_df['total_score']\n",
    "\n",
    "def train_and_evaluate_model(base_model):\n",
    "    #Create a hold-out validation set from train_df only\n",
    "    X_supervised_train, X_val, y_supervised_train, y_val = train_test_split(\n",
    "        \n",
    "        train_df.drop('Class/ASD', axis=1),\n",
    "        train_df['Class/ASD'],\n",
    "        test_size=0.4,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    #Prepare self-training data\n",
    "    # Add fake -1 to test_df again to make sure it's still 'unlabeled'\n",
    "    test_df['Class/ASD'] = -1\n",
    "\n",
    "    # Combine the reduced train and test for self-training\n",
    "    train_for_self_training = pd.concat([\n",
    "        X_supervised_train.assign(**{'Class/ASD': y_supervised_train}),\n",
    "        test_df\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    X_combined = train_for_self_training.drop('Class/ASD', axis=1)\n",
    "    y_combined = train_for_self_training['Class/ASD']\n",
    "\n",
    "    #Automatically add scaler if model type needs it\n",
    "    models_needing_scaling = (LogisticRegression,)\n",
    "    if isinstance(base_model, models_needing_scaling):\n",
    "        base_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', base_model)\n",
    "        ])\n",
    "\n",
    "    #Define and train the self-training model\n",
    "    #base_model = RandomForestClassifier(n_estimators=600, max_depth=10, min_samples_split=10)\n",
    "    self_training_model = SelfTrainingClassifier(base_model, criterion='threshold', k_best=251, threshold=.84)\n",
    "    self_training_model.fit(X_combined, y_combined)\n",
    "\n",
    "    #Evaluate on the real validation set (never seen during self-training)\n",
    "    y_pred_val_self_training = self_training_model.predict(X_val)\n",
    "\n",
    "    f1_self_training = f1_score(y_val, y_pred_val_self_training, average='macro')\n",
    "    f1_supervised = final_f1\n",
    "\n",
    "    # Calculate the uplift\n",
    "    uplift = ((f1_self_training - f1_supervised) / f1_supervised) * 100\n",
    "\n",
    "    print(\"Self-training model Macro-F1 on validation set:\", f1_self_training)\n",
    "    print (\"Supervised model Macro-F1 on validation set:\", f1_supervised)\n",
    "    print(f\"Macro-F1 Uplift/iporivment: {uplift:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb051be",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Self-Training Model vs Supervised Model Evaluation - RANDOM FOREST\n",
    "\n",
    "### Objective:\n",
    "Evaluate and compare the performance of a **self-training model** versus a **supervised-only model** based on **Macro-F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "- **Self-training model Macro-F1 on validation set**: `0.80995`\n",
    "- **Supervised model Macro-F1 on validation set**: `0.77372`\n",
    "- **Macro-F1 Uplift/Improvement**: `4.68%`\n",
    "\n",
    "### Interpretation:\n",
    "- The **self-training model** achieves a **higher Macro-F1 score** than the supervised-only model, indicating better overall performance, especially on **imbalanced data**.\n",
    "- The **4.68% uplift** shows a **fair improvement**, suggesting that the self-training approach, which incorporates both labeled and unlabeled data, enhances the modelâ€™s ability to generalize across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters:\n",
    "- **Macro-F1** ensures equal importance across all classes, preventing dominance by larger classes in imbalanced datasets.\n",
    "- **Uplift percentage** quantifies how much the self-training method improves over traditional supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b10301",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SupervisedModels/OptimizedRandomForest.ipynb\n",
    "# random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_split=10)\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b1504",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Self-Training Model vs Supervised Model Evaluation - XGBOOST\n",
    "\n",
    "### Objective:\n",
    "Evaluate and compare the performance of a **self-training model** versus a **supervised-only model** based on **Macro-F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "- **Self-training model Macro-F1 on validation set**: `0.77271`\n",
    "- **Supervised model Macro-F1 on validation set**: `0.771241`\n",
    "- **Macro-F1 Uplift/Improvement**: `00.19%`\n",
    "\n",
    "### Interpretation:\n",
    "- The **self-training model** achieves a **higher Macro-F1 score** than the supervised-only model, indicating better overall performance, especially on **imbalanced data**.\n",
    "- The **00.19% uplift** shows a **very small**, suggesting that the self-training approach, which incorporates both labeled and unlabeled data, doesnt enchnace the modelâ€™s ability to generalize across all classes very much.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters:\n",
    "- **Macro-F1** ensures equal importance across all classes, preventing dominance by larger classes in imbalanced datasets.\n",
    "- **Uplift percentage** quantifies how much the self-training method improves over traditional supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SupervisedModels/OptimizedXGBoost.ipynb\n",
    "# random forest model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=1,  # Handle imbalance through class weight\n",
    "    random_state=42\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c29f1",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Self-Training Model vs Supervised Model Evaluation - CATBOOST\n",
    "\n",
    "### Objective:\n",
    "Evaluate and compare the performance of a **self-training model** versus a **supervised-only model** based on **Macro-F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "- **Self-training model Macro-F1 on validation set**: `0.80375`\n",
    "- **Supervised model Macro-F1 on validation set**: `0.71473`\n",
    "- **Macro-F1 Uplift/Improvement**: `12.45%`\n",
    "\n",
    "### Interpretation:\n",
    "- The **self-training model** achieves a **higher Macro-F1 score** than the supervised-only model, indicating improved performance across all classes.\n",
    "- The **12.45% uplift** shows a **substantial improvement**, suggesting that the self-training approach, which utilizes both labeled and unlabeled data, greatly enhances the modelâ€™s ability to generalize and perform well on all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters:\n",
    "- **Macro-F1** ensures that the model performs well on both majority and minority classes, which is crucial for imbalanced datasets.\n",
    "- **Uplift percentage** provides a clear indication of how much the self-training method improves model performance compared to traditional supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SupervisedModels/OptimizedCatBoost.ipynb\n",
    "# random forest model\n",
    "cb_model = CatBoostClassifier(\n",
    "    random_state=42, verbose=0\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(cb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac7eab",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Self-Training Model vs Supervised Model Evaluation - LOGISTICAL REGRESSION\n",
    "\n",
    "### Objective:\n",
    "Evaluate and compare the performance of a **self-training model** versus a **supervised-only model** based on **Macro-F1 score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "- **Self-training model Macro-F1 on validation set**: `0.78430`\n",
    "- **Supervised model Macro-F1 on validation set**: `0.71384`\n",
    "- **Macro-F1 Uplift/Improvement**: `9.87%`\n",
    "\n",
    "### Interpretation:\n",
    "- The **self-training model** achieves a **higher Macro-F1 score** than the supervised-only model, indicating significantly better performance across all classes.\n",
    "- The **9.87% uplift** shows a **substantial improvement**, suggesting that the self-training approach, which uses both labeled and unlabeled data, enhances the model's ability to generalize and perform effectively, even in imbalanced data scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters:\n",
    "- **Macro-F1** ensures balanced model performance across all classes, reducing the impact of imbalanced data.\n",
    "- **Uplift percentage** demonstrates the extent of improvement achieved by the self-training method compared to traditional supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SupervisedModels/OptimizedLogisticRegression.ipynb\n",
    "# random forest model\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced', random_state=42\n",
    ")\n",
    "# Train the base model on the full training set\n",
    "train_and_evaluate_model(lr_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
