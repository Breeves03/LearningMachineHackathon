{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec14733c",
   "metadata": {},
   "source": [
    "# Supervised Random Forest Analysis\n",
    "This notebook demonstrates the use of a supervised Random Forest model for classification tasks. It includes data preprocessing, handling class imbalance, model training, cross-validation, and evaluation of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c1b2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, matthews_corrcoef, auc, average_precision_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "original_dir = os.getcwd()\n",
    "if os.path.basename(original_dir) == \"SupervisedModels\":\n",
    "    os.chdir(os.path.dirname(original_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3520302",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This section preprocesses the training and testing datasets by handling missing values and separating features from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4a481f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ShowOutput' in locals() or 'ShowOutput' in globals():\n",
    "    ShowOutput = ShowOutput\n",
    "else:\n",
    "    ShowOutput = False\n",
    "\n",
    "# Run the dataInfo notebook to preprocess the data\n",
    "\n",
    "%run dataInfo.ipynb\n",
    "\n",
    "# Combine diagnostic scores into a single feature\n",
    "score_cols = [f\"A{i}_Score\" for i in range(1, 11)]\n",
    "train_df['total_score'] = train_df[score_cols].sum(axis=1)\n",
    "test_df['total_score'] = test_df[score_cols].sum(axis=1)\n",
    "\n",
    "# Normalize the total score\n",
    "train_df['score_ratio'] = train_df['total_score'] / 10\n",
    "test_df['score_ratio'] = test_df['total_score'] / 10\n",
    "\n",
    "# Add interaction features\n",
    "train_df['gender_result'] = train_df['gender'] * train_df['result']\n",
    "train_df['age_score_ratio'] = train_df['age'] * train_df['score_ratio']\n",
    "train_df['score_autism'] = train_df['total_score'] * train_df['autism']\n",
    "train_df['age_jaundice'] = train_df['age'] * train_df['jaundice']\n",
    "train_df['autism_result'] = train_df['autism'] * train_df['result']\n",
    "train_df['gender_total_score'] = train_df['gender'] * train_df['total_score']\n",
    "\n",
    "test_df['gender_result'] = test_df['gender'] * test_df['result']\n",
    "test_df['age_score_ratio'] = test_df['age'] * test_df['score_ratio']\n",
    "test_df['score_autism'] = test_df['total_score'] * test_df['autism']\n",
    "test_df['age_jaundice'] = test_df['age'] * test_df['jaundice']\n",
    "test_df['autism_result'] = test_df['autism'] * test_df['result']\n",
    "test_df['gender_total_score'] = test_df['gender'] * test_df['total_score']\n",
    "\n",
    "# # Drop all columns that start with 'relation'\n",
    "# train_df = train_df.loc[:, ~train_df.columns.str.startswith('relation')]\n",
    "# test_df = test_df.loc[:, ~test_df.columns.str.startswith('relation')]\n",
    "\n",
    "# Load the train and test datasets\n",
    "cleanTrain = train_df.dropna()  # Drop missing values in training data\n",
    "cleanTest = test_df.dropna()  # Drop missing values in test data\n",
    "\n",
    "\n",
    "# Preprocess the train dataset\n",
    "X = cleanTrain.drop(columns=['Class/ASD'])\n",
    "y = cleanTrain['Class/ASD']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf0437",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance and Define Model\n",
    "This section applies SMOTE to address class imbalance and defines the Random Forest model with optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2fa1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "#X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Define the Random Forest model with the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=40,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Define stratified 10-fold CV with 3 repeats\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c06255",
   "metadata": {},
   "source": [
    "# Initialize Metrics\n",
    "This section initializes lists to store evaluation metrics and aggregated precision-recall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf0f9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "roc_auc_scores = []\n",
    "pr_auc_scores = []\n",
    "f1_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize lists to store aggregated precision and recall\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "PR_curve_list = []\n",
    "\n",
    "\n",
    "X_train_main, X_holdout, y_train_main, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74190adf",
   "metadata": {},
   "source": [
    "# Cross-Validation and Model Training\n",
    "This section performs stratified cross-validation, trains the model, and calculates evaluation metrics for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9737fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in cv.split(X_train_main, y_train_main):\n",
    "    # Split the original (unbalanced) data\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    else:\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Apply SMOTE only to training fold\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_val_pred = rf_model.predict(X_val)\n",
    "    y_val_pred_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "    pr_auc = np.trapz(recall, precision)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    mcc = matthews_corrcoef(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "    \n",
    "    # Append metrics\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    pr_auc_scores.append(pr_auc)\n",
    "    f1_scores.append(f1)\n",
    "    mcc_scores.append(mcc)\n",
    "    all_precision.append(precision)\n",
    "    all_recall.append(recall)\n",
    "    PR_curve_list.append((precision, recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8d317",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing and Predictions\n",
    "This section preprocesses the test dataset, aligns it with the training data, and makes predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5eceb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Copy test\n",
    "X_test = cleanTest.copy()\n",
    "\n",
    "# Step 2: Ensure all columns in training exist in test\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0  # or np.nan if imputer is used\n",
    "\n",
    "# Step 3: Reorder to match training\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "imputer.fit(X_test)\n",
    "scaler.fit(X_test)\n",
    "\n",
    "# Step 4: Preprocess\n",
    "X_test = imputer.transform(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# Step 5: Predict\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "test_probabilities = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 6: Store in test set\n",
    "cleanTest_with_predictions = cleanTest.copy()\n",
    "cleanTest_with_predictions['Class/ASD'] = test_predictions\n",
    "cleanTest_with_predictions['Probability'] = test_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005d124",
   "metadata": {},
   "source": [
    "# Final Model Training and Evaluation\n",
    "\n",
    "This section involves the final steps of training and evaluating the Random Forest model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bc4202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Fit preprocessing ONLY on training set\n",
    "X_train_proc = imputer.fit_transform(X_train_main)\n",
    "X_holdout_proc = imputer.transform(X_holdout)\n",
    "\n",
    "X_train_proc = scaler.fit_transform(X_train_proc)\n",
    "X_holdout_proc = scaler.transform(X_holdout_proc)\n",
    "\n",
    "# Step 3: Apply SMOTE only to training data\n",
    "X_resampled_final, y_resampled_final = smote.fit_resample(X_train_proc, y_train_main)\n",
    "\n",
    "# Step 4: Train final model\n",
    "rf_model.fit(X_resampled_final, y_resampled_final)\n",
    "\n",
    "# Step 5: Predict on untouched holdout\n",
    "y_holdout_pred = rf_model.predict(X_holdout_proc)\n",
    "y_holdout_proba = rf_model.predict_proba(X_holdout_proc)[:, 1]\n",
    "\n",
    "# Step 6: Compute metrics\n",
    "final_roc_auc = roc_auc_score(y_holdout, y_holdout_proba)\n",
    "final_pr_auc = average_precision_score(y_holdout, y_holdout_proba)\n",
    "final_f1 = f1_score(y_holdout, y_holdout_pred)\n",
    "final_mcc = matthews_corrcoef(y_holdout, y_holdout_pred)\n",
    "\n",
    "\n",
    "# Step 7: Print metrics\n",
    "if(ShowOutput):\n",
    "    print(\"Number of rows with a 1 in class_ASD:\", (cleanTest_with_predictions['Class/ASD'] == 1).sum())\n",
    "    print(\"Number of rows with a 0 in class_ASD:\", (cleanTest_with_predictions['Class/ASD'] == 0).sum())\n",
    "#cleanTest_with_predictions.to_csv('cleanTest_with_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea04f73",
   "metadata": {},
   "source": [
    "# Aggregate Precision-Recall Curves and Calculate Metrics\n",
    "This section aggregates precision-recall curves, calculates the area under the curve, and computes confidence intervals for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7f56743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate precision-recall curves\n",
    "mean_precision = np.linspace(0, 1, 100)\n",
    "mean_recall = np.zeros_like(mean_precision)\n",
    "\n",
    "for precision, recall in zip(all_precision, all_recall):\n",
    "    mean_recall += np.interp(mean_precision, np.flip(recall), np.flip(precision))\n",
    "mean_recall /= len(all_precision)\n",
    "\n",
    "# Calculate the area under the aggregated curve\n",
    "pr_auc = auc(mean_precision, mean_recall)\n",
    "\n",
    "# Calculate mean and confidence intervals for each metric\n",
    "def calculate_ci(scores):\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    ci_lower = mean_score - 1.96 * std_score / np.sqrt(len(scores))\n",
    "    ci_upper = mean_score + 1.96 * std_score / np.sqrt(len(scores))\n",
    "    return mean_score, ci_lower, ci_upper\n",
    "\n",
    "mean_roc_auc, ci_lower_roc_auc, ci_upper_roc_auc = calculate_ci(roc_auc_scores)\n",
    "mean_pr_auc, ci_lower_pr_auc, ci_upper_pr_auc = calculate_ci(pr_auc_scores)\n",
    "mean_f1, ci_lower_f1, ci_upper_f1 = calculate_ci(f1_scores)\n",
    "mean_mcc, ci_lower_mcc, ci_upper_mcc = calculate_ci(mcc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602866e",
   "metadata": {},
   "source": [
    "# Plot and Print Results\n",
    "This section plots the precision-recall curves and prints the evaluation metrics with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b65f5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "if(ShowOutput):\n",
    "    for i, (precision, recall) in enumerate(PR_curve_list):\n",
    "        plt.plot(recall, precision, alpha=0.3, label=f'Fold {i+1} PR Curve')\n",
    "    # Plot the aggregated curve\n",
    "    plt.plot(mean_precision, mean_recall, color='blue', lw=2, label=f'Average PR Curve (AUC = {pr_auc:.2f})')\n",
    "\n",
    "    final_precision, final_recall, _ = precision_recall_curve(y_holdout, y_holdout_proba)\n",
    "    final_avg_precision = average_precision_score(y_holdout, y_holdout_proba)\n",
    "\n",
    "    plt.plot(final_recall, final_precision, linestyle='--', color='red',\n",
    "            label=f'Final Model PR (AUC={final_avg_precision:.2f})')\n",
    "\n",
    "    plt.plot(final_recall, final_precision, linestyle='--', color='red', lw=2,\n",
    "            label=f'Final Model PR (AUC={final_avg_precision:.2f})')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title('Precision-Recall Curve (Cross-Validation)')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid()\n",
    "\n",
    "final_roc_auc = roc_auc_score(y_holdout, y_holdout_proba)\n",
    "final_pr_auc = average_precision_score(y_holdout, y_holdout_proba)\n",
    "final_f1 = f1_score(y_holdout, y_holdout_pred)\n",
    "final_mcc = matthews_corrcoef(y_holdout, y_holdout_pred)\n",
    "if(ShowOutput):    \n",
    "    print(\"\\nFinal Model Performance on Holdout Set:\")\n",
    "    print(f\"ROC-AUC: {final_roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {final_pr_auc:.4f}\")\n",
    "    print(f\"F1 Score: {final_f1:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {final_mcc:.4f}\")\n",
    "\n",
    "# print(f\"Mean ROC-AUC: {mean_roc_auc} (95% CI: {ci_lower_roc_auc}, {ci_upper_roc_auc})\")\n",
    "# print(f\"Mean PR-AUC: {mean_pr_auc} (95% CI: {ci_lower_pr_auc}, {ci_upper_pr_auc})\")\n",
    "# print(f\"Mean F1 Score: {mean_f1} (95% CI: {ci_lower_f1}, {ci_upper_f1})\")\n",
    "# print(f\"Mean Matthews Correlation Coefficient: {mean_mcc} (95% CI: {ci_lower_mcc}, {ci_upper_mcc})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
