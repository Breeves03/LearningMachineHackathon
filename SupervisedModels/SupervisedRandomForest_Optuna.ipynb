{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec14733c",
   "metadata": {},
   "source": [
    "# Supervised Random Forest Analysis\n",
    "This notebook demonstrates the use of a supervised Random Forest model for classification tasks. It includes data preprocessing, handling class imbalance, model training, cross-validation, and evaluation of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eaeff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, accuracy_score, f1_score, matthews_corrcoef, auc, average_precision_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "original_dir = os.getcwd()\n",
    "if os.path.basename(original_dir) == \"SupervisedModels\":\n",
    "    os.chdir(os.path.dirname(original_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3520302",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This section preprocesses the training and testing datasets by handling missing values and separating features from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4a481f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ShowOutput' in locals() or 'ShowOutput' in globals():\n",
    "    ShowOutput = ShowOutput\n",
    "else:\n",
    "    ShowOutput = False\n",
    "\n",
    "# Run the dataInfo notebook to preprocess the data\n",
    "\n",
    "%run dataInfo.ipynb\n",
    "\n",
    "# Combine diagnostic scores into a single feature\n",
    "score_cols = [f\"A{i}_Score\" for i in range(1, 11)]\n",
    "train_df['total_score'] = train_df[score_cols].sum(axis=1)\n",
    "test_df['total_score'] = test_df[score_cols].sum(axis=1)\n",
    "\n",
    "# Normalize the total score\n",
    "train_df['score_ratio'] = train_df['total_score'] / 10\n",
    "test_df['score_ratio'] = test_df['total_score'] / 10\n",
    "\n",
    "# Add interaction features\n",
    "train_df['gender_result'] = train_df['gender'] * train_df['result']\n",
    "train_df['age_score_ratio'] = train_df['age'] * train_df['score_ratio']\n",
    "train_df['score_autism'] = train_df['total_score'] * train_df['autism']\n",
    "train_df['age_jaundice'] = train_df['age'] * train_df['jaundice']\n",
    "train_df['autism_result'] = train_df['autism'] * train_df['result']\n",
    "train_df['gender_total_score'] = train_df['gender'] * train_df['total_score']\n",
    "\n",
    "test_df['gender_result'] = test_df['gender'] * test_df['result']\n",
    "test_df['age_score_ratio'] = test_df['age'] * test_df['score_ratio']\n",
    "test_df['score_autism'] = test_df['total_score'] * test_df['autism']\n",
    "test_df['age_jaundice'] = test_df['age'] * test_df['jaundice']\n",
    "test_df['autism_result'] = test_df['autism'] * test_df['result']\n",
    "test_df['gender_total_score'] = test_df['gender'] * test_df['total_score']\n",
    "\n",
    "# # Drop all columns that start with 'relation'\n",
    "# train_df = train_df.loc[:, ~train_df.columns.str.startswith('relation')]\n",
    "# test_df = test_df.loc[:, ~test_df.columns.str.startswith('relation')]\n",
    "\n",
    "# Load the train and test datasets\n",
    "cleanTrain = train_df.dropna()  # Drop missing values in training data\n",
    "cleanTest = test_df.dropna()  # Drop missing values in test data\n",
    "\n",
    "\n",
    "# Preprocess the train dataset\n",
    "X = cleanTrain.drop(columns=['Class/ASD'])\n",
    "y = cleanTrain['Class/ASD']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf0437",
   "metadata": {},
   "source": [
    "# Handle Class Imbalance and Define Model\n",
    "This section applies SMOTE to address class imbalance and defines the Random Forest model with optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e25e4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Define the Random Forest model with the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=40,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define stratified 10-fold CV with 3 repeats\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c06255",
   "metadata": {},
   "source": [
    "# Initialize Metrics\n",
    "This section initializes lists to store evaluation metrics and aggregated precision-recall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf0f9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "roc_auc_scores = []\n",
    "pr_auc_scores = []\n",
    "f1_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize lists to store aggregated precision and recall\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "PR_curve_list = []\n",
    "\n",
    "\n",
    "X_train_main, X_holdout, y_train_main, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74190adf",
   "metadata": {},
   "source": [
    "# Cross-Validation and Model Training\n",
    "This section performs stratified cross-validation, trains the model, and calculates evaluation metrics for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9737fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in cv.split(X_train_main, y_train_main):\n",
    "    # Split the original (unbalanced) data\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    else:\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Apply SMOTE only to training fold\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_val_pred = rf_model.predict(X_val)\n",
    "    y_val_pred_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "    pr_auc = np.trapz(recall, precision)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    mcc = matthews_corrcoef(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "    \n",
    "    # Append metrics\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    pr_auc_scores.append(pr_auc)\n",
    "    f1_scores.append(f1)\n",
    "    mcc_scores.append(mcc)\n",
    "    all_precision.append(precision)\n",
    "    all_recall.append(recall)\n",
    "    PR_curve_list.append((precision, recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8d317",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing and Predictions\n",
    "This section preprocesses the test dataset, aligns it with the training data, and makes predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5eceb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Copy test\n",
    "X_test = cleanTest.copy()\n",
    "\n",
    "# Step 2: Ensure all columns in training exist in test\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0  # or np.nan if imputer is used\n",
    "\n",
    "# Step 3: Reorder to match training\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "imputer.fit(X_test)\n",
    "scaler.fit(X_test)\n",
    "\n",
    "# Step 4: Preprocess\n",
    "X_test = imputer.transform(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# Step 5: Predict\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "test_probabilities = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 6: Store in test set\n",
    "cleanTest_with_predictions = cleanTest.copy()\n",
    "cleanTest_with_predictions['Class/ASD'] = test_predictions\n",
    "cleanTest_with_predictions['Probability'] = test_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005d124",
   "metadata": {},
   "source": [
    "# Final Model Training and Evaluation\n",
    "\n",
    "This section involves the final steps of training and evaluating the Random Forest model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc4202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Fit preprocessing ONLY on training set\n",
    "X_train_proc = imputer.fit_transform(X_train_main)\n",
    "X_holdout_proc = imputer.transform(X_holdout)\n",
    "\n",
    "X_train_proc = scaler.fit_transform(X_train_proc)\n",
    "X_holdout_proc = scaler.transform(X_holdout_proc)\n",
    "\n",
    "# Step 3: Apply SMOTE only to training data\n",
    "X_resampled_final, y_resampled_final = smote.fit_resample(X_train_proc, y_train_main)\n",
    "\n",
    "# Step 4: Train final model\n",
    "rf_model.fit(X_resampled_final, y_resampled_final)\n",
    "\n",
    "# Step 5: Predict on untouched holdout\n",
    "y_holdout_pred = rf_model.predict(X_holdout_proc)\n",
    "y_holdout_proba = rf_model.predict_proba(X_holdout_proc)[:, 1]\n",
    "\n",
    "# Step 7: Print metrics\n",
    "if(ShowOutput):\n",
    "    print(\"Number of rows with a 1 in class_ASD:\", (cleanTest_with_predictions['Class/ASD'] == 1).sum())\n",
    "    print(\"Number of rows with a 0 in class_ASD:\", (cleanTest_with_predictions['Class/ASD'] == 0).sum())\n",
    "#cleanTest_with_predictions.to_csv('cleanTest_with_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea04f73",
   "metadata": {},
   "source": [
    "# Aggregate Precision-Recall Curves and Calculate Metrics\n",
    "This section aggregates precision-recall curves, calculates the area under the curve, and computes confidence intervals for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7f56743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate precision-recall curves\n",
    "mean_precision = np.linspace(0, 1, 100)\n",
    "mean_recall = np.zeros_like(mean_precision)\n",
    "\n",
    "for precision, recall in zip(all_precision, all_recall):\n",
    "    mean_recall += np.interp(mean_precision, np.flip(recall), np.flip(precision))\n",
    "mean_recall /= len(all_precision)\n",
    "\n",
    "# Calculate the area under the aggregated curve\n",
    "pr_auc = auc(mean_precision, mean_recall)\n",
    "\n",
    "# Calculate mean and confidence intervals for each metric\n",
    "def calculate_ci(scores):\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    ci_lower = mean_score - 1.96 * std_score / np.sqrt(len(scores))\n",
    "    ci_upper = mean_score + 1.96 * std_score / np.sqrt(len(scores))\n",
    "    return mean_score, ci_lower, ci_upper\n",
    "\n",
    "mean_roc_auc, ci_lower_roc_auc, ci_upper_roc_auc = calculate_ci(roc_auc_scores)\n",
    "mean_pr_auc, ci_lower_pr_auc, ci_upper_pr_auc = calculate_ci(pr_auc_scores)\n",
    "mean_f1, ci_lower_f1, ci_upper_f1 = calculate_ci(f1_scores)\n",
    "mean_mcc, ci_lower_mcc, ci_upper_mcc = calculate_ci(mcc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602866e",
   "metadata": {},
   "source": [
    "# Plot and Print Results\n",
    "This section plots the precision-recall curves and prints the evaluation metrics with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b65f5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "if(ShowOutput):\n",
    "    for i, (precision, recall) in enumerate(PR_curve_list):\n",
    "        plt.plot(recall, precision, alpha=0.3, label=f'Fold {i+1} PR Curve')\n",
    "    # Plot the aggregated curve\n",
    "    plt.plot(mean_precision, mean_recall, color='blue', lw=2, label=f'Average PR Curve (AUC = {pr_auc:.2f})')\n",
    "\n",
    "    final_precision, final_recall, _ = precision_recall_curve(y_holdout, y_holdout_proba)\n",
    "    final_avg_precision = average_precision_score(y_holdout, y_holdout_proba)\n",
    "\n",
    "    plt.plot(final_recall, final_precision, linestyle='--', color='red',\n",
    "            label=f'Final Model PR (AUC={final_avg_precision:.2f})')\n",
    "\n",
    "    plt.plot(final_recall, final_precision, linestyle='--', color='red', lw=2,\n",
    "            label=f'Final Model PR (AUC={final_avg_precision:.2f})')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title('Precision-Recall Curve (Cross-Validation)')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid()\n",
    "\n",
    "final_roc_auc = roc_auc_score(y_holdout, y_holdout_proba)\n",
    "final_pr_auc = average_precision_score(y_holdout, y_holdout_proba)\n",
    "final_f1 = f1_score(y_holdout, y_holdout_pred)\n",
    "final_mcc = matthews_corrcoef(y_holdout, y_holdout_pred)\n",
    "accuracy = accuracy_score(y_holdout, y_holdout_pred)\n",
    "\n",
    "if(ShowOutput):    \n",
    "    print(\"\\nFinal Model Performance on Holdout Set:\")\n",
    "    print(f\"ROC-AUC: {final_roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {final_pr_auc:.4f}\")\n",
    "    print(f\"F1 Score: {final_f1:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {final_mcc:.4f}\")\n",
    "\n",
    "# print(f\"Mean ROC-AUC: {mean_roc_auc} (95% CI: {ci_lower_roc_auc}, {ci_upper_roc_auc})\")\n",
    "# print(f\"Mean PR-AUC: {mean_pr_auc} (95% CI: {ci_lower_pr_auc}, {ci_upper_pr_auc})\")\n",
    "# print(f\"Mean F1 Score: {mean_f1} (95% CI: {ci_lower_f1}, {ci_upper_f1})\")\n",
    "# print(f\"Mean Matthews Correlation Coefficient: {mean_mcc} (95% CI: {ci_lower_mcc}, {ci_upper_mcc})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "545776ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 23:19:35,817] A new study created in memory with name: no-name-18a38f82-18dd-4ccf-9a41-79262cb08ecf\n",
      "[I 2025-05-11 23:19:38,196] Trial 0 finished with value: 0.8967532414735733 and parameters: {'n_estimators': 209, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.8967532414735733.\n",
      "[I 2025-05-11 23:19:41,439] Trial 1 finished with value: 0.9031469869251681 and parameters: {'n_estimators': 481, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'class_weight': None}. Best is trial 1 with value: 0.9031469869251681.\n",
      "[I 2025-05-11 23:19:43,416] Trial 2 finished with value: 0.8938370662984767 and parameters: {'n_estimators': 139, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 'log2', 'class_weight': None}. Best is trial 1 with value: 0.9031469869251681.\n",
      "[I 2025-05-11 23:19:46,147] Trial 3 finished with value: 0.8979696475306571 and parameters: {'n_estimators': 445, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'log2', 'class_weight': None}. Best is trial 1 with value: 0.9031469869251681.\n",
      "[I 2025-05-11 23:19:48,677] Trial 4 finished with value: 0.9057885668651207 and parameters: {'n_estimators': 109, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'class_weight': None}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:50,137] Trial 5 finished with value: 0.8915390078530055 and parameters: {'n_estimators': 466, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': None, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:50,455] Trial 6 finished with value: 0.8997564107327293 and parameters: {'n_estimators': 102, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': 'log2', 'class_weight': None}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:50,774] Trial 7 finished with value: 0.8931556382319072 and parameters: {'n_estimators': 111, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:51,705] Trial 8 finished with value: 0.8946170012739252 and parameters: {'n_estimators': 400, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:52,775] Trial 9 finished with value: 0.8872010890677728 and parameters: {'n_estimators': 297, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': None, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.9057885668651207.\n",
      "[I 2025-05-11 23:19:53,723] Trial 10 finished with value: 0.9138780970695255 and parameters: {'n_estimators': 255, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 10 with value: 0.9138780970695255.\n",
      "[I 2025-05-11 23:19:54,657] Trial 11 finished with value: 0.9138780970695255 and parameters: {'n_estimators': 245, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 10 with value: 0.9138780970695255.\n",
      "[I 2025-05-11 23:19:55,504] Trial 12 finished with value: 0.9138780970695255 and parameters: {'n_estimators': 270, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 10 with value: 0.9138780970695255.\n",
      "[I 2025-05-11 23:19:56,190] Trial 13 finished with value: 0.9162060907133087 and parameters: {'n_estimators': 216, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:56,585] Trial 14 finished with value: 0.9097471287680865 and parameters: {'n_estimators': 183, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:57,289] Trial 15 finished with value: 0.912485026226778 and parameters: {'n_estimators': 338, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:58,020] Trial 16 finished with value: 0.8968140984970102 and parameters: {'n_estimators': 351, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:58,489] Trial 17 finished with value: 0.9045353746902067 and parameters: {'n_estimators': 213, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:58,919] Trial 18 finished with value: 0.9093306863515244 and parameters: {'n_estimators': 174, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:19:59,664] Trial 19 finished with value: 0.9117215906193394 and parameters: {'n_estimators': 246, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:01,095] Trial 20 finished with value: 0.8950820262069952 and parameters: {'n_estimators': 315, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': None, 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:01,997] Trial 21 finished with value: 0.9138780970695255 and parameters: {'n_estimators': 253, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:02,756] Trial 22 finished with value: 0.9100817593283599 and parameters: {'n_estimators': 229, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:03,835] Trial 23 finished with value: 0.9146525019034847 and parameters: {'n_estimators': 285, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:04,559] Trial 24 finished with value: 0.9111430413606204 and parameters: {'n_estimators': 278, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:05,375] Trial 25 finished with value: 0.9025246107262669 and parameters: {'n_estimators': 381, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'class_weight': None}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:05,805] Trial 26 finished with value: 0.9149141176032376 and parameters: {'n_estimators': 168, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': 'balanced'}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:06,562] Trial 27 finished with value: 0.8979736591089698 and parameters: {'n_estimators': 164, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None, 'class_weight': 'balanced'}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:07,114] Trial 28 finished with value: 0.9096309498133419 and parameters: {'n_estimators': 147, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'class_weight': 'balanced'}. Best is trial 13 with value: 0.9162060907133087.\n",
      "[I 2025-05-11 23:20:07,798] Trial 29 finished with value: 0.9068196557745074 and parameters: {'n_estimators': 199, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'class_weight': 'balanced'}. Best is trial 13 with value: 0.9162060907133087.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best parameters: {'n_estimators': 216, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def tune_random_forest(X, y, n_trials=30, scoring='f1'):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "        score = cross_val_score(model, X, y, scoring=scoring, cv=5, n_jobs=-1).mean()\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(\"✅ Best parameters:\", study.best_params)\n",
    "\n",
    "    final_model = RandomForestClassifier(**study.best_params)\n",
    "    final_model.fit(X, y)\n",
    "    return final_model\n",
    "\n",
    "# Train with tuning\n",
    "rf_model = tune_random_forest(X_resampled_final, y_resampled_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
