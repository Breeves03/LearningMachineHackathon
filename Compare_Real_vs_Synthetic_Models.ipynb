{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c9c808",
   "metadata": {},
   "source": [
    "# Real vs Synthetic Performance Comparison Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab19eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Blake\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load trained models and data\n",
    "%run SupervisedModels/OptimizedRandomForest.ipynb\n",
    "RF_Model_Metrics = {\n",
    "    'F1': final_f1,\n",
    "    'Roc_Auc': final_roc_auc,\n",
    "    'accuracy': accuracy\n",
    "}\n",
    "%run SupervisedModels/OptimizedXGBoost.ipynb\n",
    "XGB_Model_Metrics = {\n",
    "    'F1': final_f1,\n",
    "    'Roc_Auc': final_roc_auc,\n",
    "    'accuracy': accuracy\n",
    "}\n",
    "%run SupervisedModels/OptimizedCatBoost.ipynb\n",
    "CB_Model_Metrics = {\n",
    "    'F1': final_f1,\n",
    "    'Roc_Auc': final_roc_auc,\n",
    "    'accuracy': accuracy\n",
    "}\n",
    "%run SupervisedModels/OptimizedLogisticRegression.ipynb\n",
    "LR_Model_Metrics = {\n",
    "    'F1': final_f1,\n",
    "    'Roc_Auc': final_roc_auc,\n",
    "    'accuracy': accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data generation\n",
    "X_syn, y_syn = make_classification(n_samples=len(X), n_features=X.shape[1],\n",
    "                                   n_informative=10, n_redundant=5, n_clusters_per_class=2,\n",
    "                                   weights=[0.75, 0.25], flip_y=0.01, random_state=42)\n",
    "\n",
    "X_train_syn, X_val_syn, y_train_syn, y_val_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.2, stratify=y_syn, random_state=42\n",
    ")\n",
    "\n",
    "X_train_syn, y_train_syn = SMOTE(random_state=42).fit_resample(X_train_syn, y_train_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eeae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    'RandomForest': (rf_model, RF_Model_Metrics),\n",
    "    'XGBoost': (xgb_model, XGB_Model_Metrics),\n",
    "    # 'CatBoost': (cb_model, CB_Model_Metrics),\n",
    "    'LogisticRegression': (lr_model, LR_Model_Metrics)\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_val, y_val, metrics=None):\n",
    "    if(metrics is not None):\n",
    "        return {\n",
    "            'ROC AUC': metrics['Roc_Auc'],\n",
    "            'F1': metrics['F1'],\n",
    "            'Accuracy': metrics['accuracy']\n",
    "        }\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_prob = model.predict_proba(X_val)[:, 1]\n",
    "    return {\n",
    "        'ROC AUC': roc_auc_score(y_val, y_prob),\n",
    "        'F1': f1_score(y_val, y_pred),\n",
    "        'Accuracy': accuracy_score(y_val, y_pred)\n",
    "    }\n",
    "\n",
    "real_scores = {}\n",
    "synthetic_scores = {}\n",
    "\n",
    "# Evaluate real data models\n",
    "for name, model in models.items():\n",
    "    real_scores[name] = evaluate_model(model[0], X_holdout, y_holdout, metrics=model[1])\n",
    "\n",
    "# Retrain on synthetic and evaluate\n",
    "for name, model in models.items():\n",
    "    model[0].fit(X_train_syn, y_train_syn)\n",
    "    synthetic_scores[name] = evaluate_model(model[0], X_val_syn, y_val_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34447456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model Performance: Real vs Synthetic Comparison\n",
      "\n",
      "Model: RandomForest\n",
      "ROC AUC: Real = 0.9808, Synthetic = 0.9423, Drop = 0.0385\n",
      "F1: Real = 0.8519, Synthetic = 0.7500, Drop = 0.1019\n",
      "Accuracy: Real = 0.9375, Synthetic = 0.8875, Drop = 0.0500\n",
      "\n",
      "Model: XGBoost\n",
      "ROC AUC: Real = 0.8706, Synthetic = 0.9827, Drop = -0.1121\n",
      "F1: Real = 0.6269, Synthetic = 0.8889, Drop = -0.2620\n",
      "Accuracy: Real = 0.8438, Synthetic = 0.9437, Drop = -0.1000\n",
      "\n",
      "Model: LogisticRegression\n",
      "ROC AUC: Real = 0.8945, Synthetic = 0.8577, Drop = 0.0368\n",
      "F1: Real = 0.6216, Synthetic = 0.6214, Drop = 0.0003\n",
      "Accuracy: Real = 0.8250, Synthetic = 0.7562, Drop = 0.0687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"üîç Model Performance: Real vs Synthetic Comparison\")\n",
    "for name in models.keys():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    for metric in ['ROC AUC', 'F1', 'Accuracy']:\n",
    "        real_val = real_scores[name][metric]\n",
    "        syn_val = synthetic_scores[name][metric]\n",
    "        drop = real_val - syn_val\n",
    "        print(f\"{metric}: Real = {real_val:.4f}, Synthetic = {syn_val:.4f}, Drop = {drop:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
